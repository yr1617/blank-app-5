
"""
Streamlit ì•±: ê¸°í›„ë³€í™” - í•´ì–‘ìƒíƒœê³„ ëŒ€ì‹œë³´ë“œ
ì‘ì„±ì: ChatGPT (í•œêµ­ì–´ UI)

ìš”ì•½(ìš”êµ¬ì‚¬í•­ ì¤€ìˆ˜):
 - ìƒë‹¨: ê³µê°œ ë°ì´í„° ëŒ€ì‹œë³´ë“œ (ê³µì‹ ê³µê°œ ë°ì´í„° ì—°ê²° ì‹œë„, ì¬ì‹œë„ ë¡œì§, ì‹¤íŒ¨ ì‹œ ì˜ˆì‹œ ë°ì´í„° ìë™ ëŒ€ì²´ ë° ì•ˆë‚´)
 - í•˜ë‹¨: ì‚¬ìš©ì ì…ë ¥ ëŒ€ì‹œë³´ë“œ (í”„ë¡¬í”„íŠ¸ì— ì œê³µëœ ì„¤ëª…/ë§í¬ë§Œ ì‚¬ìš©, ì—…ë¡œë“œ ê¸ˆì§€)
 - í•œêµ­ì–´ UI, Pretendard í°íŠ¸ ì‚¬ìš© ì‹œë„, ì „ì²˜ë¦¬(ê²°ì¸¡/í˜•ë³€í™˜/ì¤‘ë³µ/ë¯¸ë˜ ë°ì´í„° ì œê±°), ìºì‹œ(@st.cache_data), CSV ë‹¤ìš´ë¡œë“œ ë²„íŠ¼ ì œê³µ
 - ë°ì´í„° í‘œì¤€í™”: date, value, group(optional)
 - ì½”ë“œ ì£¼ì„ì— ì¶œì²˜(URL) í‘œê¸°
"""
import io
import os
import time
import tempfile
import zipfile
from datetime import datetime
from typing import Tuple

import numpy as np
import pandas as pd
import plotly.express as px
import pytz
import requests
import streamlit as st

# ------------------ ì„¤ì • ------------------
LOCAL_TZ = pytz.timezone("Asia/Seoul")
TODAY = datetime.now(LOCAL_TZ).date()

# í°íŠ¸ íŒŒì¼(ì—†ìœ¼ë©´ ìë™ ìƒëµ)
PRETENDARD_PATH = "/fonts/Pretendard-Bold.ttf"

st.set_page_config(page_title="í•´ì–‘ìƒíƒœê³„ & ê¸°í›„ë³€í™” ëŒ€ì‹œë³´ë“œ", layout="wide")

# ê³µê°œ ë°ì´í„° ì¶œì²˜ (ì½”ë“œ ì£¼ì„ìœ¼ë¡œ ë‚¨ê¹€)
# - GCBD (Global Coral Bleaching Database) â€” ë…¼ë¬¸/ë©”íƒ€: https://www.nature.com/articles/s41597-022-01121-y
#   figshare ë‹¤ìš´ë¡œë“œ(ì˜ˆì‹œ): https://springernature.figshare.com/ndownloader/files/34571891
# - NOAA Coral Reef Watch: https://coralreefwatch.noaa.gov/
# - NOAA OISST (SST): https://psl.noaa.gov/data/gridded/data.noaa.oisst.v2.highres.html
# - Ocean carbon & acidification (OCADS / GLODAP): https://www.ncei.noaa.gov/products/ocean-carbon-acidification-data-system , https://glodap.info/
# - ì‚¬ìš©ì ì°¸ê³ (í”„ë¡¬í”„íŠ¸ ì œê³µ):
#   KISTI(ìš°ë¦¬ë‚˜ë¼ ì£¼ë³€ ë°”ë‹¤ ì‚°ì„±í™”): https://scienceon.kisti.re.kr/srch/selectPORSrchArticle.do?cn=JAKO202210261284373
#   NIFS ë³´ê³ ì„œ PDF: https://www.nifs.go.kr/cmmn/file/climatechange_01.pdf

# ê¶Œì¥ GCBD ë§í¬ (figshare zip/csv). ì‹¤ì œ í™˜ê²½ì— ë”°ë¼ íŒŒì¼ IDê°€ ë°”ë€” ìˆ˜ ìˆìœ¼ë‹ˆ í•„ìš”ì‹œ ê°±ì‹ .
GCBD_FIGSHARE_URLS = [
    # common figshare GCBD asset (may be zip or csv)
    "https://springernature.figshare.com/ndownloader/files/34571891",  # often a ZIP in supplementary
    "https://figshare.com/ndownloader/files/32677238",  # previously used id (fallback)
]

# ì‚¬ìš©ì ì œê³µ ì°¸ê³  ë§í¬ (í”„ë¡¬í”„íŠ¸)
KISTI_LINK = "https://scienceon.kisti.re.kr/srch/selectPORSrchArticle.do?cn=JAKO202210261284373"
NIFS_PDF_LINK = "https://www.nifs.go.kr/cmmn/file/climatechange_01.pdf"

# ------------------ ìœ í‹¸ë¦¬í‹° ------------------

def seoul_today() -> datetime.date:
    return datetime.now(LOCAL_TZ).date()

def drop_future_dates(df: pd.DataFrame, date_col: str = "date") -> pd.DataFrame:
    if date_col not in df.columns:
        return df
    df = df.copy()
    df[date_col] = pd.to_datetime(df[date_col], errors="coerce")
    df = df[~(df[date_col].dt.date > seoul_today())]
    return df

def retry_get(url: str, timeout: int = 15, retries: int = 3, backoff: float = 1.5) -> requests.Response:
    last_exc = None
    for i in range(retries):
        try:
            resp = requests.get(url, timeout=timeout)
            resp.raise_for_status()
            return resp
        except Exception as e:
            last_exc = e
            time.sleep(backoff ** (i + 1))
    raise last_exc

def standardize_timeseries(df: pd.DataFrame, date_col_candidates=None, value_col_candidates=None) -> pd.DataFrame:
    """
    ì…ë ¥ ë°ì´í„° í”„ë ˆì„ì„ (date, value, group(opt)) í˜•íƒœë¡œ ìµœëŒ€í•œ ë§¤í•‘.
    """
    df = df.copy()
    if date_col_candidates is None:
        date_col_candidates = ["date", "year", "obs_date", "survey_date", "sample_date"]
    if value_col_candidates is None:
        value_col_candidates = ["bleaching_rate", "bleaching_rate_percent", "coral_cover", "value", "percent", "bleaching"]
    # find date col
    date_col = None
    for c in date_col_candidates:
        if c in df.columns:
            date_col = c
            break
    if date_col is None:
        # try any column that looks like year or date
        for c in df.columns:
            if "year" in c.lower() or "date" in c.lower():
                date_col = c
                break
    if date_col is None:
        # create synthetic date if 'year' can't be found
        df["date"] = pd.NaT
    else:
        df["date"] = pd.to_datetime(df[date_col], errors="coerce", utc=False)
    # find value col
    value_col = None
    for c in value_col_candidates:
        if c in df.columns:
            value_col = c
            break
    if value_col is None:
        # try numeric columns other than date
        numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
        if numeric_cols:
            value_col = numeric_cols[0]
    if value_col is None:
        df["value"] = np.nan
    else:
        df["value"] = pd.to_numeric(df[value_col], errors="coerce")
    # group (optional)
    group_cols = [c for c in ["region", "site", "country", "reef_name", "taxon", "species"] if c in df.columns]
    df["group"] = df[group_cols[0]] if group_cols else None
    # cleanup: drop rows without date and value if both absent
    if "date" in df.columns:
        df = df[~(df["date"].isna() & df["value"].isna())]
    # remove duplicates
    df = df.drop_duplicates()
    # remove future dates
    df = drop_future_dates(df, date_col="date")
    return df[["date", "value", "group"]]

# ------------------ ê³µê°œ ë°ì´í„° ë¡œë“œ: GCBD ------------------

@st.cache_data(ttl=3600)
def load_gbd_from_figshare(urls=GCBD_FIGSHARE_URLS) -> Tuple[pd.DataFrame, str]:
    """
    figshare ë§í¬(ë“¤)ë¥¼ ì‹œë„í•´ì„œ GCBD CSV/ZIPì„ ë¶ˆëŸ¬ì˜´.
    ë°˜í™˜: (raw_df, used_url)
    ì˜ˆì™¸ ë°œìƒ ì‹œ ë¹ˆ DataFrameê³¼ ë¹ˆ URL ë°˜í™˜ (í˜¸ì¶œë¶€ì—ì„œ ì˜ˆì‹œ ëŒ€ì²´ ë¡œì§ ì²˜ë¦¬)
    """
    last_exc = None
    for url in urls:
        try:
            resp = retry_get(url, timeout=20, retries=3)
            content = resp.content
            ctype = resp.headers.get("content-type", "").lower()
            # If zip content or url endswith .zip -> extract first CSV
            if url.lower().endswith(".zip") or "zip" in ctype or b"PK" in content[:4]:
                with tempfile.TemporaryDirectory() as td:
                    zpath = os.path.join(td, "download.zip")
                    with open(zpath, "wb") as f:
                        f.write(content)
                    with zipfile.ZipFile(zpath, "r") as z:
                        # find CSV file(s)
                        csv_names = [n for n in z.namelist() if n.lower().endswith(".csv")]
                        if not csv_names:
                            raise RuntimeError("ZIP ë‚´ CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                        # pick first reasonable CSV
                        with z.open(csv_names[0]) as f:
                            df = pd.read_csv(f, low_memory=False)
                        return df, url
            else:
                # try read as CSV in-memory
                try:
                    df = pd.read_csv(io.BytesIO(content), low_memory=False)
                    return df, url
                except Exception:
                    # sometimes figshare returns HTML landing page; try to decode text and locate a downloadable link
                    text = content.decode("utf-8", errors="ignore")
                    # simple heuristic: find direct ndownloader link inside text
                    import re
                    m = re.search(r'https?://.*?ndownloader/files/\d+', text)
                    if m:
                        dl = m.group(0)
                        resp2 = retry_get(dl, timeout=20, retries=2)
                        df = pd.read_csv(io.BytesIO(resp2.content), low_memory=False)
                        return df, dl
                    raise
        except Exception as e:
            last_exc = e
            continue
    # all attempts failed
    raise last_exc if last_exc is not None else RuntimeError("GCBD ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨")

# ------------------ ë³´ì¡° ê³µê°œ ë°ì´í„° ë¡œë“œ (SST / ì‚°ì„±í™” ì˜ˆì‹œ) ------------------

@st.cache_data(ttl=3600)
def load_example_sst_acid():
    # ì˜ˆì‹œ ë°ì´í„° (ëŒ€ì²´ìš©) â€” ì—°/ì›” ì‹œê³„ì—´
    coral = pd.DataFrame({
        "date": pd.date_range(start="1980-01-01", periods=45, freq="Y"),
        "bleaching_rate_percent": np.clip(np.linspace(5, 65, 45) + np.random.randn(45)*3, 0, 100)
    })
    sst = pd.DataFrame({
        "date": pd.date_range(start="1980-01-01", periods=540, freq="M"),
        "sst_anomaly": np.clip(np.linspace(-0.3, 1.2, 540) + np.random.randn(540)*0.1, -2, 3)
    })
    acid = pd.DataFrame({
        "date": pd.date_range(start="1990-01-01", periods=35, freq="Y"),
        "surface_pH": np.clip(8.2 - np.linspace(0, 0.15, 35) + np.random.randn(35)*0.01, 7.7, 8.3)
    })
    return coral, sst, acid

# ------------------ ì•± UI ------------------

# Try apply Pretendard font (if available)
try:
    if os.path.exists(PRETENDARD_PATH):
        st.markdown(
            f"""
            <style>
            @font-face {{
                font-family: 'PretendardCustom';
                src: url('{PRETENDARD_PATH}');
            }}
            html, body, [class*="css"] {{
                font-family: 'PretendardCustom', sans-serif;
            }}
            </style>
            """,
            unsafe_allow_html=True,
        )
except Exception:
    pass

st.title("ğŸŒŠ í•´ì–‘ìƒíƒœê³„ & ê¸°í›„ë³€í™” ëŒ€ì‹œë³´ë“œ")
st.caption("ê³µê°œ ë°ì´í„°(ìš°ì„  GCBD) + ì‚¬ìš©ì ì…ë ¥(ë³´ê³ ì„œ ê¸°ë°˜) â€” ëª¨ë“  UIëŠ” í•œêµ­ì–´")

st.markdown("## ğŸ“Œ ê³µê°œ ë°ì´í„° ëŒ€ì‹œë³´ë“œ (GCBD ìš°ì„  ì—°ê²° ì‹œë„)")
col_left, col_right = st.columns([2, 1])

with col_right:
    st.markdown("**ë°ì´í„° ì¶œì²˜(ì‹œë„):**")
    st.write("- GCBD (Global Coral Bleaching Database) â€” ë…¼ë¬¸/ë©”íƒ€: https://www.nature.com/articles/s41597-022-01121-y")
    st.write("- Figshare (ì˜ˆì‹œ ë‹¤ìš´ë¡œë“œ): " + ", ".join(GCBD_FIGSHARE_URLS))
    st.write("- NOAA Coral Reef Watch: https://coralreefwatch.noaa.gov/")
    st.write("- NOAA OISST: https://psl.noaa.gov/data/gridded/data.noaa.oisst.v2.highres.html")
    st.write("- Ocean carbon & acidification: https://www.ncei.noaa.gov/products/ocean-carbon-acidification-data-system")
    st.write("---")
    st.markdown("**ì—°ë™/ì¸ì¦ ì°¸ê³ **")
    st.write("- Kaggle ì‚¬ìš© ì‹œ: `kaggle` íŒ¨í‚¤ì§€ì™€ API token ì„¤ì • í•„ìš” (Kaggle ì¸ì¦ íŒŒì¼ì„ í™˜ê²½ë³€ìˆ˜ ë˜ëŠ” ~/.kaggle/kaggle.jsonì— ë°°ì¹˜). ì´ ì•±ì—ì„œëŠ” kaggle API ì‚¬ìš© ì˜ˆì‹œëŠ” í¬í•¨í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

# ê³µê°œë°ì´í„° ë¡œë“œ ì‹œë„
gcbd_df = None
gcbd_source = ""
public_data_warning = None

try:
    with st.spinner("GCBD ë°ì´í„° ë‹¤ìš´ë¡œë“œ ì‹œë„ ì¤‘..."):
        raw_gcbd, gcbd_source = load_gbd_from_figshare()
        # í‘œì¤€í™”
        std_gcbd = standardize_timeseries(raw_gcbd)
        # If no useful values, raise to trigger fallback
        if std_gcbd["value"].notna().sum() < 3:
            raise RuntimeError("GCBDì—ì„œ ì˜ë¯¸ìˆëŠ” ìˆ˜ì¹˜ ì»¬ëŸ¼(value)ì„ ì°¾ì§€ ëª»í•¨")
        gcbd_df = std_gcbd
        st.success("âœ… GCBD ê³µì‹ ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬ ì™„ë£Œ")
        st.caption(f"ë°ì´í„° ì¶œì²˜: {gcbd_source}")
except Exception as e:
    # ì‹¤íŒ¨ ì‹œ: ì¬ì‹œë„ ë¡œì§ì€ load_gbd_from_figshare ë‚´ë¶€ì—ì„œ ìˆ˜í–‰
    public_data_warning = f"ê³µê°œ ë°ì´í„°(GCBD) ë¡œë“œ ì‹¤íŒ¨: {e}"
    st.warning(public_data_warning)
    # ìš”ì²­ì‚¬í•­(ì›ë˜ prompt)ì— ë”°ë¼ ì˜ˆì‹œ ë°ì´í„°ë¡œ ìë™ ëŒ€ì²´
    coral_ex, sst_ex, acid_ex = load_example_sst_acid()
    # í‘œì¤€í™”: coral_ex -> date,value
    gcbd_df = coral_ex.rename(columns={"bleaching_rate_percent":"value"})
    gcbd_df["group"] = None
    # mark that example is used
    st.info("ëŒ€ì²´ ë°ì´í„°(ì˜ˆì‹œ)ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‹œê°í™”í•©ë‹ˆë‹¤. (ì½”ë“œ ì£¼ì„ì˜ ì›ë³¸ URLì„ í™•ì¸í•˜ì„¸ìš”)")

# ê¸°ë³¸ ì „ì²˜ë¦¬: ë‚ ì§œí˜• ë³€í™˜/ì¤‘ë³µì œê±°/ë¯¸ë˜ë°ì´í„° ì œê±°
gcbd_df = gcbd_df.copy()
gcbd_df["date"] = pd.to_datetime(gcbd_df["date"], errors="coerce")
gcbd_df = gcbd_df.drop_duplicates().reset_index(drop=True)
gcbd_df = drop_future_dates(gcbd_df, date_col="date")

# ì‚¬ì´ë“œë°”: ìë™ êµ¬ì„± (ê¸°ê°„ í•„í„°, ìŠ¤ë¬´ë”© ì„ íƒ)
st.sidebar.header("ê³µê°œ ë°ì´í„° ì˜µì…˜")
min_date = gcbd_df["date"].min()
max_date = gcbd_df["date"].max()
if pd.isna(min_date) or pd.isna(max_date):
    min_date = pd.to_datetime("1980-01-01")
    max_date = seoul_today()
date_range = st.sidebar.slider("ê¸°ê°„ ì„ íƒ", min_value=min_date.date(), max_value=max_date.date(),
                               value=(min_date.date(), max_date.date()))
smoothing = st.sidebar.selectbox("ìŠ¤ë¬´ë”©(ì´ë™í‰ê· )", options=["ì‚¬ìš© ì•ˆ í•¨", "3ë…„(ë˜ëŠ” 36ê°œì›”)", "5ë…„(ë˜ëŠ” 60ê°œì›”)"], index=0)

# í•„í„° ì ìš© (ì—°ë„ ë‹¨ìœ„ í˜¹ì€ ì›” ë‹¨ìœ„ í˜¼í•© ê³ ë ¤)
mask = (gcbd_df["date"].dt.date >= date_range[0]) & (gcbd_df["date"].dt.date <= date_range[1])
plot_df = gcbd_df.loc[mask].copy()

# ìë™ ì§‘ê³„: ì—°ë„ë³„ë¡œ ìš”ì•½ (ë§Œì•½ ë°ì´í„°ê°€ ì›” ë‹¨ìœ„ì´ë©´ ì—°í‰ê· )
if (plot_df["date"].dt.freq is None) or True:
    # resample to annual by year
    plot_df["year"] = plot_df["date"].dt.year
    annual = plot_df.groupby("year")["value"].mean().reset_index()
    annual["date"] = pd.to_datetime(annual["year"].astype(str) + "-01-01")
    viz_df = annual[["date", "value"]].sort_values("date")
else:
    viz_df = plot_df[["date", "value"]].sort_values("date")

# smoothing
if smoothing != "ì‚¬ìš© ì•ˆ í•¨":
    window = 36 if "36" in smoothing else 60
    # window measured in months if monthly, else in years â€” we'll approximate by points
    viz_df["value_smoothed"] = viz_df["value"].rolling(window=3 if "3" in smoothing else 5, center=True, min_periods=1).mean()
else:
    viz_df["value_smoothed"] = viz_df["value"]

# ë©”ì¸: ì‚°í˜¸ ë°±í™”ìœ¨ ì‹œê³„ì—´
with col_left:
    st.subheader("ìµœê·¼ ì‚°í˜¸ ë°±í™” í˜„ìƒ ë¹„ìœ¨ (ì—°ë³„ ìš”ì•½)")
    if viz_df.dropna().shape[0] == 0:
        st.warning("í‘œì‹œí•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
    else:
        fig = px.line(viz_df, x="date", y="value_smoothed",
                      labels={"date":"ì—°ë„", "value_smoothed":"ë°±í™”ìœ¨ (ì„ì˜ë‹¨ìœ„)"},
                      title="ì‚°í˜¸ ë°±í™” í˜„ìƒ ë¹„ìœ¨(ì—°ë³„ í‰ê· )")
        fig.update_traces(mode="lines+markers")
        st.plotly_chart(fig, use_container_width=True)
    # CSV ë‹¤ìš´ë¡œë“œ (ì „ì²˜ë¦¬ëœ í‘œ)
    csv_bytes = viz_df.rename(columns={"value_smoothed":"value"}).to_csv(index=False).encode("utf-8")
    st.download_button("ì „ì²˜ë¦¬ëœ ì‚°í˜¸ë°±í™”_ë°ì´í„° ë‹¤ìš´ë¡œë“œ (CSV)", data=csv_bytes, file_name="gcbd_preprocessed.csv", mime="text/csv")

# ---------- ê³µê°œ ë°ì´í„° ë³´ì¡° ì‹œê°í™”: SST / ì‚°ì„±í™” (ì˜ˆì‹œ ë˜ëŠ” ì‹¤ì œ ë¡œë“œ ì‹œ ì‚¬ìš©) ----------
st.markdown("---")
st.subheader("ë³´ì¡° ì§€í‘œ: í•´ì–‘ í‘œì¸µ ìˆ˜ì˜¨(SST) & í•´ì–‘ ì‚°ì„±í™” (í‘œë³¸/ìš”ì•½)")

# ì‹œë„: ì‹¤ì œ SST/ì‚°ì„±í™” ì›ë³¸ì„ ê°€ì ¸ì˜¤ëŠ” ë³µì¡í•œ ë¡œì§ì€ ì£¼ì„ìœ¼ë¡œ ì•ˆë‚´.
st.write("ì°¸ê³ : NOAA OISST ë“± ì›ìë£ŒëŠ” ëŒ€ìš©ëŸ‰(netCDF)ìœ¼ë¡œ ì œê³µë©ë‹ˆë‹¤. ì‹¤ì„œë¹„ìŠ¤ì—ì„œëŠ” xarray.open_datasetì„ í™œìš©í•´ ì§€ì—­/ê¸°ê°„ì„ ì¶”ì¶œí•˜ì„¸ìš”. ì—¬ê¸°ì„œëŠ” ì˜ˆì‹œ/ìš”ì•½ ë°ì´í„°ë¡œ í‘œì‹œí•©ë‹ˆë‹¤.")

# load example series (or derive from earlier example)
if public_data_warning:
    coral_ex, sst_ex, acid_ex = load_example_sst_acid()
    sst_plot = sst_ex.copy()
    acid_plot = acid_ex.copy()
else:
    # if GCBD came with additional columns, create simple derived SST/acid series if possible
    sst_plot = pd.DataFrame({"date": pd.date_range(start=gcbd_df["date"].min(), end=gcbd_df["date"].max(), freq="Y")})
    sst_plot["sst_anomaly"] = np.linspace(-0.2, 0.9, len(sst_plot))
    acid_plot = pd.DataFrame({"date": pd.date_range(start=sst_plot["date"].min(), periods=len(sst_plot), freq="Y")})
    acid_plot["surface_pH"] = 8.2 - np.linspace(0, 0.12, len(acid_plot))

col_sst, col_acid = st.columns(2)
with col_sst:
    fig_sst = px.area(sst_plot, x="date", y="sst_anomaly",
                      labels={"date":"ì—°ë„", "sst_anomaly":"SST ì´ìƒì¹˜ (Â°C)"},
                      title="í•´ì–‘ í‘œì¸µ ìˆ˜ì˜¨ ì´ìƒì¹˜ (ì˜ˆì‹œ/ìš”ì•½)")
    st.plotly_chart(fig_sst, use_container_width=True)
    st.download_button("SST_ì‹œê³„ì—´_ë‹¤ìš´ë¡œë“œ (CSV)", data=sst_plot.to_csv(index=False).encode("utf-8"),
                       file_name="sst_timeseries.csv", mime="text/csv")
with col_acid:
    fig_acid = px.line(acid_plot, x="date", y="surface_pH",
                       labels={"date":"ì—°ë„", "surface_pH":"í‘œì¸µ pH"},
                       title="í‘œì¸µ pH ì¶”ì„¸ (ì˜ˆì‹œ/ìš”ì•½)")
    st.plotly_chart(fig_acid, use_container_width=True)
    st.download_button("ì‚°ì„±í™”_ì‹œê³„ì—´_ë‹¤ìš´ë¡œë“œ (CSV)", data=acid_plot.to_csv(index=False).encode("utf-8"),
                       file_name="ocean_acidification_timeseries.csv", mime="text/csv")

# ------------------ ì‚¬ìš©ì ì…ë ¥(ë³´ê³ ì„œ) ê¸°ë°˜ ëŒ€ì‹œë³´ë“œ ------------------
st.markdown("---")
st.header("ğŸ“ ì‚¬ìš©ì ì…ë ¥ ê¸°ë°˜ ëŒ€ì‹œë³´ë“œ (ë³´ê³ ì„œ ë‚´ìš©ë§Œ ì‚¬ìš©, ì—…ë¡œë“œ ì—†ìŒ)")
st.markdown("**ë³´ê³ ì„œ ìš”ì•½(ì…ë ¥ ë‚´ìš©)**")
st.write("ì œëª©(ê°€ì œ): ì—­ëŒ€ ìµœì•…ì˜ ë°”ë‹¤ ê·¸ë¦¬ê³  ë” ìµœì•…ì´ ë  ë°”ë‹¤.")
st.write("ìš”ì•½: ì‚°í˜¸ ë°±í™”, í•´ì–‘ ì‚°ì„±í™”, ê³ ìˆ˜ì˜¨ìœ¼ë¡œ ì¸í•œ ì–´ë¥˜ íì‚¬ ë“± â€” ë³¸ë¬¸ê³¼ ì°¸ê³ ìë£Œ(ë§í¬ í¬í•¨)ê°€ ì œê³µë¨.")

# ë³¸ë¡ 1: ìµœê·¼ 45ë…„ê°„ ì‚°í˜¸ ë°±í™” ë¹„ìœ¨ â€” ì´ë¯¸ ê³µê°œë°ì´í„°(ìƒë‹¨ GCBD ë˜ëŠ” ì˜ˆì‹œ)ë¥¼ ì‚¬ìš©
st.subheader("ë³¸ë¡  1 â€” ìµœê·¼ 45ë…„ê°„ ì‚°í˜¸ ë°±í™” í˜„ìƒ ë¹„ìœ¨ (ë³´ê³ ì„œìš©)")
# For report visuals, take last 45 years from gcbd_df (if possible) otherwise use example
report_df = gcbd_df.copy()
if report_df["date"].dtype == "object" or report_df["date"].isna().all():
    # fallback synthetic
    report_df = pd.DataFrame({"date": pd.date_range(start="1980-01-01", periods=45, freq="Y"),
                              "value": np.clip(np.linspace(5, 65, 45) + np.random.randn(45)*3, 0, 100)})
report_df = report_df.sort_values("date").reset_index(drop=True)
if len(report_df) >= 45:
    report_plot = report_df.tail(45).copy()
else:
    # pad/extend synthetic if needed
    report_plot = report_df.copy()

fig_report = px.area(report_plot, x="date", y="value",
                     labels={"date":"ì—°ë„", "value":"ì‚°í˜¸ ë°±í™”ìœ¨ (ì„ì˜ë‹¨ìœ„)"},
                     title="ë³´ê³ ì„œìš©: ìµœê·¼ 45ë…„ ì‚°í˜¸ ë°±í™”ìœ¨ (ì—°ë³„)")
st.plotly_chart(fig_report, use_container_width=True)
st.download_button("ë³´ê³ ì„œ_ì‚°í˜¸ë°±í™”_ë°ì´í„° ë‹¤ìš´ë¡œë“œ (CSV)", data=report_plot.to_csv(index=False).encode("utf-8"),
                   file_name="report_coral_45y.csv", mime="text/csv")

# ë³¸ë¡ 2: í•´ì–‘ ì‚°ì„±í™”, ê³ ìˆ˜ì˜¨, ì–´ë¥˜ íì‚¬ ì˜í–¥ â€” ë³µí•© ì‹œê°í™” (ë³´ê³ ì„œ í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë§Œë“  ìš”ì•½/í•©ì„± ì§€í‘œ)
st.subheader("ë³¸ë¡  2 â€” í•´ì–‘ ì‚°ì„±í™”Â·ê³ ìˆ˜ì˜¨Â·ì–´ë¥˜ íì‚¬ ì˜í–¥ (ë³µí•© ì‹œê°í™”)")
# Merge sst_plot & acid_plot & coral for a combined view (annual)
try:
    sst_a = sst_plot.copy()
    sst_a["date"] = pd.to_datetime(sst_a["date"], errors="coerce")
    sst_a["year"] = sst_a["date"].dt.year
    sst_ann = sst_a.groupby("year")["sst_anomaly"].mean().reset_index()
    sst_ann["date"] = pd.to_datetime(sst_ann["year"].astype(str) + "-01-01")
except Exception:
    sst_ann = pd.DataFrame({"date": pd.date_range(start="1980-01-01", periods=35, freq="Y"), "sst_anomaly": np.linspace(-0.2, 0.9, 35)})

acid_a = acid_plot.copy()
acid_a["date"] = pd.to_datetime(acid_a["date"], errors="coerce")
acid_a["year"] = acid_a["date"].dt.year
acid_ann = acid_a.groupby("year")["surface_pH"].mean().reset_index()
acid_ann["date"] = pd.to_datetime(acid_ann["year"].astype(str) + "-01-01")

merge_base = pd.merge(report_plot.rename(columns={"value":"bleaching_rate_percent"})[["date","bleaching_rate_percent"]],
                      sst_ann[["date","sst_anomaly"]], on="date", how="outer")
merge_base = pd.merge(merge_base, acid_ann[["date","surface_pH"]], on="date", how="outer")
merge_base = merge_base.sort_values("date").reset_index(drop=True)

# í•©ì„± ì–´ë¥˜ íì‚¬ ì§€ìˆ˜ (ê°„ë‹¨í•œ ê°€ì¤‘ì¹˜ ëª¨ë¸)
merge_base["sst_norm"] = (merge_base["sst_anomaly"] - merge_base["sst_anomaly"].min()) / (merge_base["sst_anomaly"].max() - merge_base["sst_anomaly"].min() + 1e-9)
merge_base["pH_drop"] = 8.2 - merge_base["surface_pH"]
merge_base["pH_norm"] = (merge_base["pH_drop"] - merge_base["pH_drop"].min()) / (merge_base["pH_drop"].max() - merge_base["pH_drop"].min() + 1e-9)
merge_base["fish_mortality_index"] = (0.7 * merge_base["sst_norm"].fillna(0) + 0.3 * merge_base["pH_norm"].fillna(0)) * 100
merge_base = drop_future_dates(merge_base, date_col="date")

# Plot combined with multiple traces
fig_comb = px.line(merge_base, x="date", y="bleaching_rate_percent", labels={"date":"ì—°ë„", "bleaching_rate_percent":"ë°±í™”ìœ¨ (%)"}, title="í•´ì–‘ ê³ ìˆ˜ì˜¨ Â· ì‚°ì„±í™” Â· ì–´ë¥˜ íì‚¬(í•©ì„±ì§€ìˆ˜) ë¹„êµ")
fig_comb.add_scatter(x=merge_base["date"], y=merge_base["sst_anomaly"], mode="lines+markers", name="SST ì—°í‰ê·  ì´ìƒì¹˜ (Â°C)")
fig_comb.add_scatter(x=merge_base["date"], y=merge_base["fish_mortality_index"], mode="lines+markers", name="ì–´ë¥˜ íì‚¬ ì§€ìˆ˜ (í•©ì„±)")
st.plotly_chart(fig_comb, use_container_width=True)
st.download_button("ë³¸ë¡ 2_ë³µí•©_ì „ì²˜ë¦¬_ë°ì´í„° ë‹¤ìš´ë¡œë“œ (CSV)", data=merge_base.to_csv(index=False).encode("utf-8"),
                   file_name="report_combined_ocean_data.csv", mime="text/csv")

# ------------------ ê²°ë¡  / ì°¸ê³ ìë£Œ ------------------
st.markdown("---")
st.subheader("ê²°ë¡  ë° ê¶Œê³  (ìë™ ì œì•ˆ)")
st.write(
    "- ì‚°í˜¸ ë°±í™” í˜„ìƒê³¼ í•´ì–‘ ì‚°ì„±í™”, ê³ ìˆ˜ì˜¨ì€ ìƒí˜¸ ì—°ê²°ë˜ì–´ í•´ì–‘ìƒíƒœê³„ì— ì‹¬ê°í•œ ì˜í–¥ì„ ì¤ë‹ˆë‹¤.\n"
    "- ê¶Œê³ : ì˜¨ì‹¤ê°€ìŠ¤ ê°ì¶•, í•´ì–‘ ë³´í˜¸êµ¬ì—­ í™•ëŒ€, ì‚°í˜¸ ë³µì›, ì¥ê¸° ëª¨ë‹ˆí„°ë§ ì²´ê³„ êµ¬ì¶• ë“±.\n"
)
st.markdown("**ì°¸ê³ ìë£Œ(ì•±ì—ì„œ ì‹œë„/ì°¸ì¡°í•œ ë§í¬)**")
st.write(f"- GCBD ë…¼ë¬¸/ë°ì´í„°(ì°¸ê³ ): https://www.nature.com/articles/s41597-022-01121-y")
st.write(f"- Figshare ë‹¤ìš´ë¡œë“œ(ì‹œë„): {', '.join(GCBD_FIGSHARE_URLS)}")
st.write(f"- KISTI(í•œêµ­ ê´€ë ¨ ë…¼ë¬¸): {KISTI_LINK}")
st.write(f"- NIFS ë³´ê³ ì„œ PDF: {NIFS_PDF_LINK}")
st.caption("ì•± ë…¸íŠ¸: ê³µê°œ ë°ì´í„°ëŠ” ì›ë¬¸ í¬ë§·(ì˜ˆ: netCDF, ZIP, ëŒ€ìš©ëŸ‰ CSV)ìœ¼ë¡œ ì œê³µë˜ëŠ” ê²½ìš°ê°€ ë§ì•„, ì‹¤ì„œë¹„ìŠ¤ í™˜ê²½ì—ì„œëŠ” ì¸ì¦Â·ë‹¤ìš´ë¡œë“œÂ·ì „ì²˜ë¦¬(ì˜ˆ: xarray.open_dataset) ë‹¨ê³„ë¥¼ ì¶”ê°€í•˜ì„¸ìš”.")
