# streamlit_app.py
"""
Streamlit ì•±: ê¸°í›„ë³€í™” - í•´ì–‘ìƒíƒœê³„ ëŒ€ì‹œë³´ë“œ
ì‘ì„±ì: ChatGPT (í•œêµ­ì–´ UI)
- ê³µê°œ ë°ì´í„° ìš°ì„  ì‹œë„: NOAA Coral Reef Watch (ERDDAP / CSV), NOAA OISST (ERDDAP) ë“±
- ì‹¤íŒ¨ ì‹œ: ì˜ˆì‹œ(í•©ì„±/ìƒ˜í”Œ) ë°ì´í„°ë¡œ ìë™ ëŒ€ì²´ ë° í™”ë©´ ì•ˆë‚´
- í•œêµ­ì–´ UI, Pretendard ì‹œë„, ì „ì²˜ë¦¬(ê²°ì¸¡/í˜•ë³€í™˜/ì¤‘ë³µ/ë¯¸ë˜ë°ì´í„° ì œê±°), ìºì‹œ(@st.cache_data), CSV ë‹¤ìš´ë¡œë“œ ì œê³µ
- ë°ì´í„° í‘œì¤€í™”: date, value, group(optional)
ì£¼ì˜: ì‹¤ì œ ì„œë¹„ìŠ¤ í™˜ê²½ì—ì„œëŠ” netCDF/ëŒ€ìš©ëŸ‰ ìë£Œ ì²˜ë¦¬ë¥¼ ìœ„í•´ xarray + dask ì‚¬ìš©ì„ ê¶Œì¥í•©ë‹ˆë‹¤.
ì¶œì²˜(ì°¸ê³  URL - ì½”ë“œ ì£¼ì„ì— ëª…ì‹œ):
 - NOAA Coral Reef Watch (CRW) data resources / ERDDAP instructions:
   https://coralreefwatch.noaa.gov/  and https://coralreefwatch.noaa.gov/instructions/Accessing_Coral_Reef_Watch_Data_via_Data_Servers_at_CoastWatch_20240403.pdf
 - NOAA OISST (Optimum Interpolation SST) / ERDDAP access:
   https://psl.noaa.gov/data/gridded/data.noaa.oisst.v2.highres.html  and ERDDAP CSV endpoints (example): https://coastwatch.pfeg.noaa.gov/erddap/
 - Ocean Carbon & Acidification (OCADS) / GLODAP general pages:
   https://www.ncei.noaa.gov/products/ocean-carbon-acidification-data-system
   https://glodap.info/
"""

import io
import os
import time
import zipfile
import tempfile
from datetime import datetime
from typing import Tuple

import numpy as np
import pandas as pd
import plotly.express as px
import pytz
import requests
import streamlit as st

# ------------------ ì„¤ì • ------------------
LOCAL_TZ = pytz.timezone("Asia/Seoul")
TODAY = datetime.now(LOCAL_TZ).date()

# í°íŠ¸ íŒŒì¼(ì—†ìœ¼ë©´ ìë™ ìƒëµ)
PRETENDARD_PATH = "/fonts/Pretendard-Bold.ttf"

st.set_page_config(page_title="í•´ì–‘ìƒíƒœê³„ & ê¸°í›„ë³€í™” ëŒ€ì‹œë³´ë“œ", layout="wide")

# ------------------ ìœ í‹¸ë¦¬í‹° ------------------

def seoul_today() -> datetime.date:
    return datetime.now(LOCAL_TZ).date()

def drop_future_dates(df: pd.DataFrame, date_col: str = "date") -> pd.DataFrame:
    if date_col not in df.columns:
        return df
    df = df.copy()
    df[date_col] = pd.to_datetime(df[date_col], errors="coerce")
    df = df[~(df[date_col].dt.date > seoul_today())]
    return df

def retry_get(url: str, timeout: int = 15, retries: int = 3, backoff: float = 1.5) -> requests.Response:
    last_exc = None
    for i in range(retries):
        try:
            resp = requests.get(url, timeout=timeout)
            resp.raise_for_status()
            return resp
        except Exception as e:
            last_exc = e
            time.sleep(backoff ** (i + 1))
    raise last_exc

def standardize_timeseries(df: pd.DataFrame, date_col_candidates=None, value_col_candidates=None) -> pd.DataFrame:
    """
    ì…ë ¥ ë°ì´í„° í”„ë ˆì„ì„ (date, value, group(opt)) í˜•íƒœë¡œ ìµœëŒ€í•œ ë§¤í•‘.
    ì „ì²˜ë¦¬: ê²°ì¸¡ ì²˜ë¦¬, í˜•ë³€í™˜, ì¤‘ë³µ ì œê±°, ë¯¸ë˜ ë°ì´í„° ì œê±°
    """
    df = df.copy()
    if date_col_candidates is None:
        date_col_candidates = ["date", "time", "datetime", "year", "obs_date", "survey_date", "sample_date"]
    if value_col_candidates is None:
        value_col_candidates = ["bleaching_rate", "bleaching_rate_percent", "coral_cover", "value", "percent", "sst", "sst_anomaly", "surface_pH"]
    # find date col
    date_col = None
    for c in date_col_candidates:
        if c in df.columns:
            date_col = c
            break
    if date_col is None:
        for c in df.columns:
            if "date" in c.lower() or "time" in c.lower() or "year" in c.lower():
                date_col = c
                break
    if date_col is None:
        df["date"] = pd.NaT
    else:
        df["date"] = pd.to_datetime(df[date_col], errors="coerce")
    # find value col
    value_col = None
    for c in value_col_candidates:
        if c in df.columns:
            value_col = c
            break
    if value_col is None:
        numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
        if numeric_cols:
            value_col = numeric_cols[0]
    if value_col is None:
        df["value"] = np.nan
    else:
        df["value"] = pd.to_numeric(df[value_col], errors="coerce")
    # group (optional)
    group_cols = [c for c in ["region", "site", "country", "reef_name", "taxon", "species"] if c in df.columns]
    df["group"] = df[group_cols[0]] if group_cols else None
    # drop rows without date & value both
    if "date" in df.columns:
        df = df[~(df["date"].isna() & df["value"].isna())]
    df = df.drop_duplicates().reset_index(drop=True)
    df = drop_future_dates(df, date_col="date")
    # ensure columns order
    cols = ["date", "value", "group"]
    for c in cols:
        if c not in df.columns:
            df[c] = np.nan
    return df[["date", "value", "group"]]

# ------------------ ê³µê°œ ë°ì´í„° ë¡œë“œ ì‹œë„: NOAA CRW (ERDDAP) ë° OISST (ERDDAP) ------------------

@st.cache_data(ttl=3600)
def load_crw_from_erddap(sample_point_lat: float = 0.0, sample_point_lon: float = 0.0) -> Tuple[pd.DataFrame, str]:
    """
    NOAA Coral Reef Watch (CRW) ERDDAP ì ‘ê·¼ì„ ì‹œë„í•˜ì—¬ 'Degree Heating Week' ë˜ëŠ” 'bleaching' ê´€ë ¨ ì‹œê³„ì—´ì„ CSVë¡œ ê°€ì ¸ì˜µë‹ˆë‹¤.
    êµ¬í˜„ì€ ERDDAPì˜ /csv endpoint ì‚¬ìš©ì„ ì‹œë„í•©ë‹ˆë‹¤. (ì‚¬ìš©ì í™˜ê²½/ë„¤íŠ¸ì›Œí¬/ERDDAP êµ¬ì„±ì— ë”°ë¼ ì‹¤íŒ¨ ê°€ëŠ¥)
    ë°˜í™˜: (raw_df, used_url) ë˜ëŠ” ì˜ˆì™¸ ë°œìƒ
    ì°¸ê³ (ì‚¬ìš©ì ì½ê¸°ìš©): CRW ERDDAP ì ‘ê·¼ ë°©ë²•: https://coralreefwatch.noaa.gov/instructions/Accessing_Coral_Reef_Watch_Data_via_Data_Servers_at_CoastWatch_20240403.pdf
    """
    # ERDDAP ì„œë¹„ìŠ¤ ì˜ˆì‹œ ì—”ë“œí¬ì¸íŠ¸ (í™˜ê²½ì— ë”°ë¼ ë³€ê²½ í•„ìš”)
    # ì•„ë˜ëŠ” ERDDAPì˜ ì‹œê°„-ìœ„ì¹˜ ê¸°ë°˜ CSV ì¶”ì¶œ ì˜ˆì‹œ í¬ë§·ì…ë‹ˆë‹¤.
    # - ì‹¤ì œë¡œëŠ” ê´€ì‹¬ ì§€ì (ìœ„ë„/ê²½ë„)ì´ë‚˜ ì˜ì—­, ë³€ìˆ˜ëª…ì„ ì •í™•íˆ ì•Œê³  ìš”ì²­í•´ì•¼ í•©ë‹ˆë‹¤.
    # - ì—¬ê¸°ì„œëŠ” 'virtual station'ì‹ ë‹¨ì¼ í”½ì…€ ì‹œê³„ì—´ ìš”ì²­ì„ ì‹œë„í•˜ëŠ” ì˜ˆì‹œ URLì„ êµ¬ì„±í•©ë‹ˆë‹¤.
    # NOTE: ì—¬ëŸ¬ ERDDAP ì¸ìŠ¤í„´ìŠ¤ê°€ ì¡´ì¬í•˜ë¯€ë¡œ ì•„ë˜ URLì€ í™˜ê²½/ì‹œì ì— ë”°ë¼ ë™ì‘í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    # ì˜ˆì‹œ (CoastWatch/NOAA): CSV í˜•ì‹ìœ¼ë¡œ ìš”ì²­í•˜ë©´ CSV ì‘ë‹µì„ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    # (ì‚¬ìš© ì‹¤íŒ¨ ì‹œ ì˜ˆì™¸ë¥¼ ë°œìƒì‹œí‚¤ê³  ìƒìœ„ì—ì„œ ëŒ€ì²´ ë°ì´í„° ì‚¬ìš©)
    erddap_examples = [
        # NOAA CRW data server (public): use the 'satellite' time series virtual station CSV example
        "https://coastwatch.pfeg.noaa.gov/erddap/griddap/noaa_oisst_v2_avhrr.csv?sst[1982-01-01T00:00:00Z:1:2024-12-31T00:00:00Z][({lat}):1:({lat})][({lon}):1:({lon})]".format(lat=sample_point_lat, lon=sample_point_lon),
        # Generic CRW ERDDAP endpoints may exist; attempt a common host (may 404)
        "https://coastwatch.pfeg.noaa.gov/erddap/tabledap/coralreefwatch.csv",
    ]
    last_exc = None
    for url in erddap_examples:
        try:
            resp = retry_get(url, timeout=25, retries=2)
            # some ERDDAP CSVs include header/units lines; read with pandas
            df = pd.read_csv(io.BytesIO(resp.content), low_memory=False)
            return df, url
        except Exception as e:
            last_exc = e
            continue
    raise last_exc if last_exc is not None else RuntimeError("CRW ERDDAP ì ‘ê·¼ ì‹¤íŒ¨")

@st.cache_data(ttl=3600)
def load_oisst_timeseries_via_erddap(bbox=None, start="1982-01-01", end=None) -> Tuple[pd.DataFrame, str]:
    """
    NOAA OISST(ì˜ˆ: monthly or daily) ë°ì´í„°ë¥¼ ERDDAP CSVë¡œ ì‹œë„í•´ ì›ê²©ì— ì ‘ê·¼.
    bbox: (min_lat, max_lat, min_lon, max_lon) í˜¹ì€ ë‹¨ì¼ í”½ì…€ (lat, lon) íŠœí”Œ
    ë°˜í™˜: (df, url) ë˜ëŠ” ì˜ˆì™¸ ë°œìƒ
    ì¶œì²˜: https://psl.noaa.gov/data/gridded/data.noaa.oisst.v2.highres.html
    """
    if end is None:
        end = seoul_today().isoformat()
    # ERDDAP griddap ì˜ˆì‹œ (dataset id, ë³€ìˆ˜ëª… ë“±ì€ ì„œë¹„ìŠ¤ì— ë”°ë¼ ë‹¬ë¼ì§)
    # ì—¬ê¸°ì„œëŠ” 'noaa_oisst_v2_avhrr' (ì˜ˆì‹œ) ì˜ .csv ìš”ì²­ í¬ë§·ì„ ì‚¬ìš©
    try:
        if bbox and len(bbox) == 4:
            min_lat, max_lat, min_lon, max_lon = bbox
            # CSV request (ì¼ë³„/ì›”ë³„ ì‹œê³„ì—´ ì¶”ì¶œ) - ì´ URLì€ ERDDAP ì¸ìŠ¤í„´ìŠ¤ì— ë”°ë¼ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆìŒ.
            url = (
                f"https://coastwatch.pfeg.noaa.gov/erddap/griddap/noaa_oisst_v2_avhrr.csv?"
                f"sst[{start}T00:00:00Z:1:{end}T00:00:00Z][({min_lat}):1:({max_lat})][({min_lon}):1:({max_lon})]"
            )
        else:
            # fallback: global timeseries aggregated example (may fail)
            url = f"https://coastwatch.pfeg.noaa.gov/erddap/griddap/noaa_oisst_v2_avhrr.csv?sst[{start}T00:00:00Z:1:{end}T00:00:00Z]"
        resp = retry_get(url, timeout=25, retries=2)
        df = pd.read_csv(io.BytesIO(resp.content), low_memory=False)
        return df, url
    except Exception as e:
        raise

@st.cache_data(ttl=3600)
def load_public_ocean_data():
    """
    ê³µê°œ ë°ì´í„° ë¡œë“œ ì‹œë„: 1) NOAA Coral Reef Watch via ERDDAP 2) NOAA OISST via ERDDAP
    ì¬ì‹œë„/ì˜ˆì™¸ ì²˜ë¦¬ í¬í•¨. ì‹¤íŒ¨ ì‹œ ì˜ˆì‹œ ë°ì´í„°(í•©ì„±/ìƒ˜í”Œ)ë¥¼ ë°˜í™˜.
    """
    # ì‹œë„ 1: Coral Reef Watch (point-based or table)
    try:
        # sample virtual station (ì ë‹¹í•œ ìœ„ë„/ê²½ë„ëŠ” ì„ì˜ë¡œ 0,0 ì‚¬ìš© â€” ì‹¤ì œë¡œëŠ” ê´€ì‹¬ ì§€ì  ì§€ì • í•„ìš”)
        raw1, src1 = load_crw_from_erddap(sample_point_lat=0.0, sample_point_lon=0.0)
        df1 = standardize_timeseries(raw1)
        # require at least some numeric values
        if df1["value"].notna().sum() >= 3:
            return {"type": "crw", "df": df1, "source": src1}
    except Exception:
        pass
    # ì‹œë„ 2: OISST (SST) via ERDDAP
    try:
        raw2, src2 = load_oisst_timeseries_via_erddap(bbox=(-10, 10, 120, 150), start="1982-01-01")
        # OISST CSV êµ¬ì¡°ê°€ (time, lat, lon, sst) ì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ê°„ë‹¨íˆ í‰ê·  ì‹œê³„ì—´ ìƒì„±
        df2 = raw2.copy()
        # try common column names
        time_col = None
        for c in ["time", "date", "t"]:
            if c in df2.columns:
                time_col = c
                break
        if time_col is None:
            time_col = df2.columns[0]
        df2["date"] = pd.to_datetime(df2[time_col], errors="coerce")
        numeric_cols = df2.select_dtypes(include=[np.number]).columns.tolist()
        valcol = None
        for v in ["sst", "sst_anomaly", "sea_surface_temperature"]:
            if v in df2.columns:
                valcol = v
                break
        if valcol is None and numeric_cols:
            valcol = numeric_cols[0]
        df2["value"] = pd.to_numeric(df2[valcol], errors="coerce")
        df2 = df2[["date", "value"]]
        df2 = drop_future_dates(df2, date_col="date")
        df2 = standardize_timeseries(df2)
        if df2["value"].notna().sum() >= 3:
            return {"type": "oisst", "df": df2, "source": src2}
    except Exception:
        pass
    # ëª¨ë“  ê³µê°œ ë°ì´í„° ì‹œë„ ì‹¤íŒ¨ -> ì˜ˆì‹œ ë°ì´í„° ë°˜í™˜
    coral = pd.DataFrame({
        "date": pd.date_range(start="1980-01-01", periods=45, freq="Y"),
        "value": np.clip(np.linspace(5, 65, 45) + np.random.randn(45)*3, 0, 100)
    })
    sst = pd.DataFrame({
        "date": pd.date_range(start="1980-01-01", periods=540, freq="M"),
        "value": np.clip(np.linspace(-0.3, 1.2, 540) + np.random.randn(540)*0.1, -2, 3)
    })
    acid = pd.DataFrame({
        "date": pd.date_range(start="1990-01-01", periods=35, freq="Y"),
        "value": np.clip(8.2 - np.linspace(0, 0.15, 35) + np.random.randn(35)*0.01, 7.7, 8.3)
    })
    return {"type": "example", "df_coral": coral, "df_sst": sst, "df_acid": acid, "source": "ì˜ˆì‹œ ë°ì´í„° ì‚¬ìš©"}

# ------------------ UI ì‹œì‘ ------------------

# Pretendard ì ìš© ì‹œë„ (ìˆìœ¼ë©´ ì‚¬ìš©)
try:
    if os.path.exists(PRETENDARD_PATH):
        st.markdown(
            f"""
            <style>
            @font-face {{
                font-family: 'PretendardCustom';
                src: url('{PRETENDARD_PATH}');
            }}
            html, body, [class*="css"] {{
                font-family: 'PretendardCustom', sans-serif;
            }}
            </style>
            """,
            unsafe_allow_html=True,
        )
except Exception:
    pass

st.title("ğŸŒŠ í•´ì–‘ìƒíƒœê³„ & ê¸°í›„ë³€í™” ëŒ€ì‹œë³´ë“œ")
st.caption("ê³µê°œ ë°ì´í„°(ìš°ì„  NOAA ê³„ì—´ ì‹œë„) + ì‚¬ìš©ì ì…ë ¥(ë³´ê³ ì„œ ê¸°ë°˜) â€” í•œêµ­ì–´ UI")

st.markdown("## ğŸ“Œ ê³µê°œ ë°ì´í„° ëŒ€ì‹œë³´ë“œ (ê³µì‹ ê³µê°œ ë°ì´í„° ìš°ì„  ì‹œë„)")
col_left, col_right = st.columns([2, 1])

with col_right:
    st.markdown("**ì‹œë„í•œ ê³µê°œ ë°ì´í„° ì¶œì²˜(ì˜ˆì‹œ)**")
    st.write("- NOAA Coral Reef Watch (CRW) â€” ERDDAP / ì‹œê°„-ìœ„ì¹˜ ê¸°ë°˜ ì‹œê³„ì—´. (ë¬¸ì„œ/ERDDAP ì ‘ê·¼ ê¶Œì¥)")
    st.write("- NOAA OISST (Optimum Interpolation SST) â€” ERDDAP / netCDF ëŒ€í˜• ì‹œê³„ì—´ (ìš”ì•½/ì›”ë³„ ì‚¬ìš© ê°€ëŠ¥)")
    st.write("- Ocean Carbon & Acidification (OCADS) / GLODAP (pHÂ·ì‚°ì„±í™” ê´€ë ¨ ê´€ì¸¡)")
    st.write("---")
    st.markdown("**ì°¸ê³ /ê¶Œê³ **")
    st.write("- ëŒ€ìš©ëŸ‰ ì›ìë£Œ(netCDF)ëŠ” xarray.open_dataset + ì§€ì—­/ê¸°ê°„ ì„œë¸Œì…‹ ì‚¬ìš© ê¶Œì¥")
    st.write("- ë§Œì•½ Kaggle ë°ì´í„° ì‚¬ìš© ì‹œ: kaggle CLI ì¸ì¦(https://www.kaggle.com/docs/api) í•„ìš”")

# ê³µê°œ ë°ì´í„° ë¡œë“œ ì‹œë„
public_result = load_public_ocean_data()
public_data_warning = None

if public_result.get("type") == "example":
    st.warning("ê³µê°œ ë°ì´í„° ìë™ ì—°ê²° ì‹œë„ì— ì‹¤íŒ¨í•˜ì—¬ ì˜ˆì‹œ(ëŒ€ì²´) ë°ì´í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. (ì•±ì€ ê³µê°œ ë°ì´í„° ì—°ê²°ì„ ë‹¤ì‹œ ì‹œë„í•˜ë„ë¡ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤.)")
    coral_df = public_result["df_coral"].rename(columns={"value":"value"})
    sst_df = public_result["df_sst"].rename(columns={"value":"value"})
    acid_df = public_result["df_acid"].rename(columns={"value":"value"})
    gc_source = public_result.get("source", "ì˜ˆì‹œ")
else:
    if public_result["type"] == "crw":
        st.success("âœ… NOAA Coral Reef Watch(ERDDAP) ë°ì´í„° ë¡œë“œ ì„±ê³µ (ê°€ëŠ¥í•œ ê²½ìš°)")
    else:
        st.success("âœ… NOAA OISST(ERDDAP) ë°ì´í„° ë¡œë“œ ì„±ê³µ (ê°€ëŠ¥í•œ ê²½ìš°)")
    gc_source = public_result.get("source", "")
    base_df = public_result["df"].copy()
    # í‘œì¤€í™”
    std = standardize_timeseries(base_df)
    # split into plausible series: if variable looks like SST or pH, route to sst/acid, else coral
    if std["value"].notna().mean() >= 0:
        # simple heuristic: if values usually between 0-40 -> assume bleaching %; if around 7-9 -> pH; if -2..3 -> sst
        vmean = std["value"].median()
        if vmean >= -1 and vmean <= 4:
            sst_df = std.rename(columns={"value":"value"})[["date","value","group"]]
            coral_df = pd.DataFrame({"date":[], "value":[]})
            acid_df = pd.DataFrame({"date":[], "value":[]})
        elif vmean >= 6 and vmean <= 9:
            acid_df = std.rename(columns={"value":"value"})[["date","value","group"]]
            coral_df = pd.DataFrame({"date":[], "value":[]})
            sst_df = pd.DataFrame({"date":[], "value":[]})
        else:
            coral_df = std.rename(columns={"value":"value"})[["date","value","group"]]
            sst_df = pd.DataFrame({"date":[], "value":[]})
            acid_df = pd.DataFrame({"date":[], "value":[]})

# ê¸°ë³¸ ì „ì²˜ë¦¬: ë‚ ì§œí˜•, ì¤‘ë³µ ì œê±°, ë¯¸ë˜ ë°ì´í„° ì œê±°
def finalize_df(df: pd.DataFrame):
    df = df.copy()
    if "date" in df.columns:
        df["date"] = pd.to_datetime(df["date"], errors="coerce")
    df = df.drop_duplicates().reset_index(drop=True)
    df = drop_future_dates(df, date_col="date")
    return df

coral_df = finalize_df(coral_df)
sst_df = finalize_df(sst_df)
acid_df = finalize_df(acid_df)

# ì‚¬ì´ë“œë°”: ê³µê°œ ë°ì´í„° ì˜µì…˜ (ìë™ êµ¬ì„±)
st.sidebar.header("ê³µê°œ ë°ì´í„° ì˜µì…˜")
# determine global min/max across available series
all_dates = pd.concat([coral_df["date"], sst_df["date"], acid_df["date"]]).dropna()
if all_dates.empty:
    min_date = pd.to_datetime("1980-01-01")
    max_date = seoul_today()
else:
    min_date = all_dates.min()
    max_date = all_dates.max()
date_range = st.sidebar.slider("ê¸°ê°„ ì„ íƒ", min_value=min_date.date(), max_value=max_date.date(),
                               value=(min_date.date(), max_date.date()))
smoothing = st.sidebar.selectbox("ìŠ¤ë¬´ë”©(ì´ë™í‰ê· )", options=["ì‚¬ìš© ì•ˆ í•¨", "3ë…„(ì—°ê³„)", "5ë…„(ì—°ê³„)"], index=0)

# ë©”ì¸: ì‚°í˜¸ ë°±í™” ì‹œê³„ì—´ (ì—°ë³„ ìš”ì•½)
with col_left:
    st.subheader("ìµœê·¼ ì‚°í˜¸ ë°±í™” í˜„ìƒ ë¹„ìœ¨ (ì—°ë³„ ìš”ì•½)")
    if coral_df.empty:
        st.info("ê³µê°œë°ì´í„°ì—ì„œ ì§ì ‘ ì‚°í˜¸ ë°±í™” ë¹„ìœ¨ ì‹œê³„ì—´ì„ í™•ë³´í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì˜ˆì‹œ/ëŒ€ì²´ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ê±°ë‚˜, NOAA Coral Reef Watchì—ì„œ 'virtual station' ë˜ëŠ” QCed ê´€ì¸¡ íŒŒì¼ì„ ERDDAPë¡œ ì¶”ì¶œí•´ ë³´ì„¸ìš”.")
        # create example for visualization
        viz_df = pd.DataFrame({"date": pd.date_range(start="1980-01-01", periods=45, freq="Y"),
                               "value": np.clip(np.linspace(5, 65, 45) + np.random.randn(45)*3, 0, 100)})
    else:
        # filter by date_range
        mask = (coral_df["date"].dt.date >= date_range[0]) & (coral_df["date"].dt.date <= date_range[1])
        plot_df = coral_df.loc[mask].copy()
        # aggregate to annual if mixed freq
        plot_df["year"] = plot_df["date"].dt.year
        annual = plot_df.groupby("year")["value"].mean().reset_index()
        annual["date"] = pd.to_datetime(annual["year"].astype(str) + "-01-01")
        viz_df = annual[["date","value"]].sort_values("date")
    # smoothing
    if smoothing != "ì‚¬ìš© ì•ˆ í•¨":
        window = 3 if "3" in smoothing else 5
        viz_df["value_smoothed"] = viz_df["value"].rolling(window=window, center=True, min_periods=1).mean()
    else:
        viz_df["value_smoothed"] = viz_df["value"]
    if viz_df.dropna().shape[0] == 0:
        st.warning("í‘œì‹œí•  ì‚°í˜¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
    else:
        fig = px.line(viz_df, x="date", y="value_smoothed",
                      labels={"date":"ì—°ë„", "value_smoothed":"ì‚°í˜¸ ë°±í™”ìœ¨ (ì„ì˜ë‹¨ìœ„)"},
                      title="ì‚°í˜¸ ë°±í™” í˜„ìƒ ë¹„ìœ¨ (ì—°ë³„ í‰ê· , ì „ì²˜ë¦¬ëœ ê°’)")
        fig.update_traces(mode="lines+markers")
        st.plotly_chart(fig, use_container_width=True)
    st.download_button("ì „ì²˜ë¦¬ëœ ì‚°í˜¸ë°±í™”_ë°ì´í„° ë‹¤ìš´ë¡œë“œ (CSV)",
                       data=viz_df.rename(columns={"value_smoothed":"value"}).to_csv(index=False).encode("utf-8"),
                       file_name="coral_preprocessed.csv", mime="text/csv")

# ë³´ì¡° ì§€í‘œ: SST ë° í•´ì–‘ ì‚°ì„±í™” (pH)
st.markdown("---")
st.subheader("ë³´ì¡° ì§€í‘œ: í•´ì–‘ í‘œì¸µ ìˆ˜ì˜¨(SST) & í‘œì¸µ pH(ì‚°ì„±í™”)")

col_sst, col_acid = st.columns(2)
with col_sst:
    if sst_df.empty:
        st.info("SST(í•´ìˆ˜ì˜¨) ê³µê°œë°ì´í„°ë¥¼ í™•ë³´í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì˜ˆì‹œ ì‹œê³„ì—´ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        sst_plot = pd.DataFrame({"date": pd.date_range(start="1980-01-01", periods=540, freq="M"),
                                 "sst_anomaly": np.clip(np.linspace(-0.3, 1.2, 540) + np.random.randn(540)*0.1, -2, 3)})
    else:
        mask = (sst_df["date"].dt.date >= date_range[0]) & (sst_df["date"].dt.date <= date_range[1])
        tmp = sst_df.loc[mask].copy()
        # if monthly/daily -> convert to annual mean for plotting consistency
        tmp["year"] = tmp["date"].dt.year
        sst_plot = tmp.groupby("year")["value"].mean().reset_index()
        sst_plot["date"] = pd.to_datetime(sst_plot["year"].astype(str) + "-01-01")
        sst_plot = sst_plot.rename(columns={"value":"sst_anomaly"})
    fig_sst = px.area(sst_plot, x="date", y=sst_plot.columns[1],
                      labels={"date":"ì—°ë„", sst_plot.columns[1]:"SST ì´ìƒì¹˜ (ì„ì˜ë‹¨ìœ„, Â°C)"},
                      title="í•´ì–‘ í‘œì¸µ ìˆ˜ì˜¨ ì´ìƒì¹˜ (ì˜ˆì‹œ/ìš”ì•½)")
    st.plotly_chart(fig_sst, use_container_width=True)
    st.download_button("SST_ì‹œê³„ì—´_ë‹¤ìš´ë¡œë“œ (CSV)", data=sst_plot.to_csv(index=False).encode("utf-8"),
                       file_name="sst_timeseries.csv", mime="text/csv")

with col_acid:
    if acid_df.empty:
        st.info("í‘œì¸µ pH ê³µê°œë°ì´í„°ë¥¼ í™•ë³´í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì˜ˆì‹œ ì‹œê³„ì—´ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        acid_plot = pd.DataFrame({"date": pd.date_range(start="1990-01-01", periods=35, freq="Y"),
                                  "surface_pH": np.clip(8.2 - np.linspace(0, 0.15, 35) + np.random.randn(35)*0.01, 7.7, 8.3)})
    else:
        mask = (acid_df["date"].dt.date >= date_range[0]) & (acid_df["date"].dt.date <= date_range[1])
        tmp = acid_df.loc[mask].copy()
        tmp["year"] = tmp["date"].dt.year
        acid_plot = tmp.groupby("year")["value"].mean().reset_index()
        acid_plot["date"] = pd.to_datetime(acid_plot["year"].astype(str) + "-01-01")
        acid_plot = acid_plot.rename(columns={"value":"surface_pH"})
    fig_acid = px.line(acid_plot, x="date", y=acid_plot.columns[1],
                       labels={"date":"ì—°ë„", acid_plot.columns[1]:"í‘œì¸µ pH"},
                       title="í‘œì¸µ pH ì¶”ì„¸ (ì˜ˆì‹œ/ìš”ì•½)")
    st.plotly_chart(fig_acid, use_container_width=True)
    st.download_button("ì‚°ì„±í™”_ì‹œê³„ì—´_ë‹¤ìš´ë¡œë“œ (CSV)", data=acid_plot.to_csv(index=False).encode("utf-8"),
                       file_name="ocean_acidification_timeseries.csv", mime="text/csv")

# ------------------ ì‚¬ìš©ì ì…ë ¥(ë³´ê³ ì„œ) ê¸°ë°˜ ëŒ€ì‹œë³´ë“œ (ì—…ë¡œë“œ ì—†ìŒ: í”„ë¡¬í”„íŠ¸ ë‚´ í…ìŠ¤íŠ¸ë§Œ ì‚¬ìš©) ------------------
st.markdown("---")
st.header("ğŸ“ ì‚¬ìš©ì ì…ë ¥ ê¸°ë°˜ ëŒ€ì‹œë³´ë“œ (ë³´ê³ ì„œ ë‚´ìš©ë§Œ ì‚¬ìš©, ì—…ë¡œë“œ ì—†ìŒ)")

st.markdown("**ë³´ê³ ì„œ ì›ë¬¸(ìš”ì•½)**")
st.write("ì œëª©(ê°€ì œ): ì—­ëŒ€ ìµœì•…ì˜ ë°”ë‹¤ ê·¸ë¦¬ê³  ë” ìµœì•…ì´ ë  ë°”ë‹¤.")
st.write("ì„œë¡ : ìµœê·¼ ìˆ˜ì‹­ë…„ê°„ ì§€êµ¬ ì˜¨ë‚œí™”ê°€ ê°€ì†í™”ë¨ì— ë”°ë¼ í•´ìˆ˜ì˜¨ ìƒìŠ¹, ì‚°í˜¸ ë°±í™”, í•´ì–‘ ì‚°ì„±í™” ë“±ì´ í•´ì–‘ìƒíƒœê³„ì— ì‹¬ê°í•œ ì˜í–¥ì„ ì£¼ê³  ìˆìŠµë‹ˆë‹¤.")
st.write("ë³¸ë¡  ìš”ì•½: ì‚°í˜¸ ë°±í™”, ê³ ìˆ˜ì˜¨ì— ì˜í•œ ì–´ë¥˜ íì‚¬, í•´ì–‘ ì‚°ì„±í™”ì˜ ì•…í™” ë“± â€” ê¶Œê³ : ì˜¨ì‹¤ê°€ìŠ¤ ê°ì¶•, í•´ì–‘ë³´í˜¸êµ¬ì—­ í™•ëŒ€, ì‚°í˜¸ ë³µì›, ì¥ê¸° ëª¨ë‹ˆí„°ë§ ë“±.")

# ë³¸ë¡ 1: ìµœê·¼ 45ë…„ê°„ ì‚°í˜¸ ë°±í™” ë¹„ìœ¨ (ë³´ê³ ì„œìš©) â€” ìƒë‹¨ ê³µê°œë°ì´í„°(ë˜ëŠ” ì˜ˆì‹œ) ì‚¬ìš©
st.subheader("ë³¸ë¡  1 â€” ìµœê·¼ 45ë…„ê°„ ì‚°í˜¸ ë°±í™” ë¹„ìœ¨ (ë³´ê³ ì„œìš©)")
# generate report_df from coral_df or example
report_df = coral_df.copy()
if report_df.empty or report_df["date"].isna().all():
    report_df = pd.DataFrame({"date": pd.date_range(start="1980-01-01", periods=45, freq="Y"),
                              "value": np.clip(np.linspace(5, 80, 45) + np.random.randn(45)*4, 0, 100)})
report_df = report_df.sort_values("date").reset_index(drop=True)
report_plot = report_df.tail(45).copy()
fig_report = px.area(report_plot, x="date", y="value",
                     labels={"date":"ì—°ë„", "value":"ì‚°í˜¸ ë°±í™”ìœ¨ (ì„ì˜ë‹¨ìœ„)"},
                     title="ë³´ê³ ì„œìš©: ìµœê·¼ 45ë…„ ì‚°í˜¸ ë°±í™”ìœ¨ (ì—°ë³„)")
st.plotly_chart(fig_report, use_container_width=True)
st.download_button("ë³´ê³ ì„œ_ì‚°í˜¸ë°±í™”_ë°ì´í„° ë‹¤ìš´ë¡œë“œ (CSV)", data=report_plot.to_csv(index=False).encode("utf-8"),
                   file_name="report_coral_45y.csv", mime="text/csv")

# ë³¸ë¡ 2: í•´ì–‘ ì‚°ì„±í™”Â·ê³ ìˆ˜ì˜¨Â·ì–´ë¥˜ íì‚¬ ì˜í–¥ (ë³µí•© ì‹œê°í™”)
st.subheader("ë³¸ë¡  2 â€” í•´ì–‘ ì‚°ì„±í™”Â·ê³ ìˆ˜ì˜¨Â·ì–´ë¥˜ íì‚¬ ì˜í–¥ (ë³µí•© ì‹œê°í™”)")
# ë³‘í•©: report_plot + sst_plot(ì—°í‰ê· ) + acid_plot(ì—°í‰ê· )
def to_annual(df, value_name):
    df2 = df.copy()
    df2["date"] = pd.to_datetime(df2["date"], errors="coerce")
    df2 = df2.dropna(subset=["date"])
    df2["year"] = df2["date"].dt.year
    ann = df2.groupby("year").agg({value_name: "mean"}).reset_index()
    ann["date"] = pd.to_datetime(ann["year"].astype(str) + "-01-01")
    return ann[["date", value_name]]

r = report_plot.rename(columns={"value":"bleaching_rate_percent"})[["date","bleaching_rate_percent"]].copy()
sst_ann = to_annual(sst_plot.rename(columns={sst_plot.columns[1]:"sst_anomaly"}), "sst_anomaly") if not sst_plot.empty else pd.DataFrame({"date":[],"sst_anomaly":[]})
acid_ann = to_annual(acid_plot.rename(columns={acid_plot.columns[1]:"surface_pH"}), "surface_pH") if not acid_plot.empty else pd.DataFrame({"date":[],"surface_pH":[]})

merge_base = pd.merge(r, sst_ann, on="date", how="outer")
merge_base = pd.merge(merge_base, acid_ann, on="date", how="outer")
merge_base = merge_base.sort_values("date").reset_index(drop=True)
merge_base = drop_future_dates(merge_base, date_col="date")

# í•©ì„± ì–´ë¥˜ íì‚¬ ì§€ìˆ˜ (ê°„ë‹¨ ê°€ì¤‘ì¹˜ ëª¨ë¸): SST ìƒìŠ¹ ë° pH í•˜ë½ì„ ì´ìš©
merge_base["sst_norm"] = (merge_base["sst_anomaly"] - merge_base["sst_anomaly"].min()) / (merge_base["sst_anomaly"].max() - merge_base["sst_anomaly"].min() + 1e-9) if "sst_anomaly" in merge_base.columns else 0
merge_base["pH_drop"] = 8.2 - merge_base.get("surface_pH", 8.2)
merge_base["pH_norm"] = (merge_base["pH_drop"] - merge_base["pH_drop"].min()) / (merge_base["pH_drop"].max() - merge_base["pH_drop"].min() + 1e-9) if "surface_pH" in merge_base.columns else 0
merge_base["fish_mortality_index"] = (0.7 * merge_base["sst_norm"].fillna(0) + 0.3 * merge_base["pH_norm"].fillna(0)) * 100

fig_comb = px.line(merge_base, x="date", y="bleaching_rate_percent", labels={"date":"ì—°ë„", "bleaching_rate_percent":"ë°±í™”ìœ¨ (%)"},
                   title="í•´ì–‘ ê³ ìˆ˜ì˜¨ Â· ì‚°ì„±í™” Â· ì–´ë¥˜ íì‚¬(í•©ì„±ì§€ìˆ˜) ë¹„êµ")
if "sst_anomaly" in merge_base.columns:
    fig_comb.add_scatter(x=merge_base["date"], y=merge_base["sst_anomaly"], mode="lines+markers", name="SST ì—°í‰ê·  ì´ìƒì¹˜ (ì„ì˜ë‹¨ìœ„ Â°C)")
fig_comb.add_scatter(x=merge_base["date"], y=merge_base["fish_mortality_index"], mode="lines+markers", name="ì–´ë¥˜ íì‚¬ ì§€ìˆ˜ (í•©ì„±)")
st.plotly_chart(fig_comb, use_container_width=True)
st.download_button("ë³¸ë¡ 2_ë³µí•©_ì „ì²˜ë¦¬_ë°ì´í„° ë‹¤ìš´ë¡œë“œ (CSV)", data=merge_base.to_csv(index=False).encode("utf-8"),
                   file_name="report_combined_ocean_data.csv", mime="text/csv")

# ------------------ ê²°ë¡  / ì°¸ê³ ìë£Œ ------------------
st.markdown("---")
st.subheader("ê²°ë¡  ë° ê¶Œê³  (ìë™ ì œì•ˆ)")
st.write(
    "- ì‚°í˜¸ ë°±í™”, í•´ì–‘ ì‚°ì„±í™”, ê³ ìˆ˜ì˜¨ì€ ìƒí˜¸ ì—°ê³„ë˜ì–´ í•´ì–‘ìƒíƒœê³„ì— ì‹¬ê°í•œ ì˜í–¥ì„ ì¤ë‹ˆë‹¤.\n"
    "- ê¶Œê³ : ì˜¨ì‹¤ê°€ìŠ¤ ê°ì¶•, í•´ì–‘ ë³´í˜¸êµ¬ì—­ í™•ëŒ€, ì‚°í˜¸ ë³µì›, ì¥ê¸° ëª¨ë‹ˆí„°ë§ ì²´ê³„ êµ¬ì¶• ë“±.\n"
)
st.markdown("**ì°¸ê³ ìë£Œ(ì•±ì—ì„œ ì‹œë„/ì°¸ì¡°í•œ ë§í¬)**")
st.write("- NOAA Coral Reef Watch: https://coralreefwatch.noaa.gov/ . (CRW ERDDAP ì ‘ê·¼ ì•ˆë‚´ ë¬¸ì„œ ê¶Œì¥)")
st.write("- NOAA OISST (SST): https://psl.noaa.gov/data/gridded/data.noaa.oisst.v2.highres.html")
st.write("- Ocean Carbon & Acidification (OCADS): https://www.ncei.noaa.gov/products/ocean-carbon-acidification-data-system")
st.write("- GLODAP: https://glodap.info/")
st.caption("ì•± ë…¸íŠ¸: ê³µê°œ ë°ì´í„°ëŠ” ì¢…ì¢… netCDF/ZIP/ëŒ€ìš©ëŸ‰ CSV í˜•ì‹ìœ¼ë¡œ ì œê³µë©ë‹ˆë‹¤. ì‹¤ì„œë¹„ìŠ¤ í™˜ê²½ì—ì„œëŠ” ì¸ì¦Â·ë‹¤ìš´ë¡œë“œÂ·ì „ì²˜ë¦¬(xarray.open_dataset ë“±)ë¥¼ ì¶”ê°€í•˜ì„¸ìš”.")
